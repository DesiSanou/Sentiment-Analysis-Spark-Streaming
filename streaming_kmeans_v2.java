
package twitter.analysis.streaming.kmeans.model;

import org.apache.spark.mllib.clustering.StreamingKMeans;
import org.apache.spark.mllib.clustering.StreamingKMeansModel;
import org.apache.spark.ml.evaluation.ClusteringEvaluator;

import org.apache.spark.ml.feature.HashingTF;
import org.apache.spark.ml.feature.Tokenizer;

import org.apache.spark.ml.Pipeline;
import org.apache.spark.ml.PipelineModel;
import org.apache.spark.ml.PipelineStage;

import org.apache.spark.ml.linalg.Vector;
import scala.Tuple2;

import com.databricks.ml.local.ModelExport;

public class JavaKMeansModel{


    public Tuple2<PipelineModel, double> trainModel(JavaDStream<Vector> dataset, StreamingKMeansModel latestModel ) throws Exception  {
        private final static IntWritable numClusters = new IntWritable(3);
        private final static IntWritable numDimensions = new IntWritable(1);

        Vector[] lastCenters = latestmodel.clusterCenters();
        double[] lastWeigths = latestmodel.clusterWeights();

        Tokenizer tokenizer = new Tokenizer()
        .setInputCol("Text")
        .setOutputCol("words");

        HashingTF hashingTF = new HashingTF()
        .setNumFeatures(1000)
        .setInputCol(tokenizer.getOutputCol())
        .setOutputCol("featureVector");

        StreamingKMeans kmModel = new StreamingKMeans();

        kmModel.setK(numClusters);
        kmModel.setDecayFactor(0.5);   // valeur de alpha, facteur d'oubli
        kmModel.setInitialCenters(lastCenters, lastWeights);


        Pipeline pipeline = new Pipeline()
        .setStages(new PipelineStage[] {tokenizer, hashingTF, kmModel});

        PipelineModel newModel = pipeline.fit(dataset);

        // Evaluate clustering by computing Silhouette score
        Dataset<Row> predictions = model.transform(test);
        ClusteringEvaluator evaluator =  new ClusteringEvaluator()
        .setFeaturesCol("featureVector")
        .setPredictionCol("cluster")
        .setMetricName("silhouette");

        double silhouette = evaluator.evaluate(predictions);


        return new Tuple2<PipelineModel, double>(newModel, silhouette)
    }

    public JavaDStream<Integer> makePrediction(JavaDStream<Vector> scrappedDataBatch, PipelineModel model) {
        JavaDStream<Integer> sentiments = model.transform(scrappedDataBatch);
        return sentiments;
    }

    public void public static void main(String[] args) {

        Logger.getLogger("org").setLevel(Level.OFF);
        Logger.getLogger("akka").setLevel(Level.OFF);

        // Create the context with a 1 second batch size 
        SparkConf sparkConf = new SparkConf().setAppName("SentAnalyStreaming").setMaster("local[2]"); 
        JavaStreamingContext jsc = new JavaStreamingContext(sparkConf, Durations.seconds(5)); 

        Dataset<Vector> centersDf = jsc.read().json("./centers.json");
        Vector[] firstCenters = centersDf.col("centers");
        double[] firstWeigths ;
        FileReader fr = (new FileReader("savedSilhouette.txt")); 
  
        double savedSilhouette = Double.parseDouble( (new FileReader("savedSilhouette.txt")).read()).or(-1); 
  

        //PipelineModel pipelineModel = PipelineModel.read.load("currentModel");
        StreamingKMeansModel firstModel =PipelineModel.read.load("currentModel").stages.last.asInstanceOf[StreamingKMeansModel].or(new StreamingKMeansModel(firstCenters, 1.0)); //weights);
        JavaDStream<String> historyText;
        
        jsc.checkpoint(".");


        // Create a JavaReceiverInputDStream on target ip:port and count the 
        // words in input stream of \n delimited text (eg. generated by 'nc') 
        JavaReceiverInputDStream<String> lines = jsc.socketTextStream("localhost", 9999); 
        

        JavaDStream<String> text = lines.cache();

        int iTunePeriod = 100;
        int tuningTimeRemain = iTunePeriod;
        if(tuningTimeRemain==0){
            StreamingKMeansModel latestModel = currentModel.stages.last.asInstanceOf[StreamingKMeansModel].or(firstModel);
            double latestSilhouette = currentSilhouette.or(savedSilhouette);

            Tuple2<PipelineModel, double> model_and_silhouette = trainModel(historyText, latestModel);
            double currentSilhouette = model_and_silhouette._2;

            if(currentSilhouette > latestSilhouette){
                currentModel= model_and_silhouette._1;
            }

            tuningTimeRemain = iTunePeriod; 
        }
        else{

            historyText.union(text);
            
            JavaDStream<Integer> predictions= currentModel.makePrediction(text);
            tuningPeriod=tuningPeriod - 1;
        }
         
        jsc.start(); 
        try { 
            jsc.awaitTermination(); 
        } catch (InterruptedException e) {
            currentModel.write.save("currentModel");
            PrintWriter out = new PrintWriter("currentSilhouette.txt");
            out.println(currentSilhouette.toString());

            e.printStackTrace(); 
        } 
        
    }

}